{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a semantic search indexing function using Chonkie and doc2vec, follow these steps:\n",
    "\n",
    "1. **Install necessary libraries**:\n",
    "   - `Chonkie` for chunking text.\n",
    "   - `gensim` for doc2vec.\n",
    "   - `PyMuPDF` for importing PDF files.\n",
    "\n",
    "2. **Import the PDF file and extract text**.\n",
    "3. **Chunk the text into semantically similar chunks**.\n",
    "4. **Encode the chunks into a vector space using doc2vec**.\n",
    "\n",
    "Here's a step-by-step implementation:\n",
    "\n",
    "### Step 1: Install necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sh"
    }
   },
   "outputs": [],
   "source": [
    "pip install chonkie gensim pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 2: Import the PDF file and extract text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 3: Chunk the text into semantically similar chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import Chonkie\n",
    "\n",
    "def chunk_text(text):\n",
    "    chonkie = Chonkie()\n",
    "    chunks = chonkie.chunk(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 4: Encode the chunks into a vector space using doc2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "def encode_chunks(chunks):\n",
    "    tagged_data = [TaggedDocument(words=chunk.split(), tags=[str(i)]) for i, chunk in enumerate(chunks)]\n",
    "    model = Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    return model\n",
    "\n",
    "def vectorize_chunks(model, chunks):\n",
    "    vectors = [model.infer_vector(chunk.split()) for chunk in chunks]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Putting it all together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_semantic_search_index(pdf_path):\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = chunk_text(text)\n",
    "    model = encode_chunks(chunks)\n",
    "    vectors = vectorize_chunks(model, chunks)\n",
    "    return vectors, model\n",
    "\n",
    "# Example usage\n",
    "pdf_path = 'path/to/your/pdf/file.pdf'\n",
    "vectors, model = create_semantic_search_index(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This code will:\n",
    "1. Extract text from a PDF file.\n",
    "2. Chunk the text into semantically similar chunks.\n",
    "3. Encode the chunks into a vector space using doc2vec.\n",
    "\n",
    "You can then use the `vectors` and `model` for semantic search indexing."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
