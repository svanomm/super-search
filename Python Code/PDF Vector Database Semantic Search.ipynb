{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvicinity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vicinity\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import random \n",
    "import re\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "from pypdf import PdfReader\n",
    "from time import perf_counter\n",
    "from chonkie import SemanticChunker\n",
    "from model2vec import StaticModel\n",
    "from vicinity import Vicinity\n",
    "from numpy import linalg\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "main = r\"C:/Users\\Steven\\Documents/Python\\Semantic Indexing Tool\"\n",
    "data = r\"C:/Users\\Steven\\Documents/Python\\Data/NBER papers\"\n",
    "test_file = f\"{data}/32362.pdf\"\n",
    "\n",
    "indexes = f'{main}/indexes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "model = StaticModel.from_pretrained(\"minishlab/potion-base-8M\")\n",
    "\n",
    "# Basic initialization with default parameters\n",
    "chunker = SemanticChunker(\n",
    "    embedding_model=\"minishlab/potion-base-8M\",  # Default model\n",
    "    threshold='auto',                            # Similarity threshold (0-1) or (1-100) or \"auto\"\n",
    "    chunk_size=512,                              # Maximum tokens per chunk\n",
    "    min_sentences=1,                             # Initial sentences per chunk\n",
    "    similarity_window=3,                         # Number of sentences to compare for similarity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future goals:\n",
    "#### PDF extraction:\n",
    "- identify elements from the paper to exclude, such as the references pages\n",
    "- identify the abstract to take advantage of its summary function.\n",
    "    - idea: what if we finetune an LLM to input a paper and output the abstract? Could be neat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Words\n",
    "stop_words = [\n",
    "      'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves'\n",
    "    , 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours'\n",
    "    , 'yourself', 'yourselves', 'he', 'him', 'his', 'himself'\n",
    "    , 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself'\n",
    "    , 'they', 'them', 'their', 'theirs', 'themselves', 'what'\n",
    "    , 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were'\n",
    "    , 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did'\n",
    "    , 'doing', 'a', 'an', 'the', 'and', 'because'\n",
    "    , 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about'\n",
    "    , 'from', 'so', 'very', 'should', \"should've\"\n",
    "    ]\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\r', ' ')\n",
    "    for word in stop_words:\n",
    "        d = word.title()\n",
    "        text = text.replace(f' {word} ', ' ')\n",
    "        text = text.replace(f' {d} ', ' ')\n",
    "    text = text.replace('  ', ' ')\n",
    "    text = text.replace('  ', ' ')\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert PDF to chunkable text\n",
    "def prepare_PDF(in_path):\n",
    "    reader = PdfReader(in_path)\n",
    "\n",
    "    # combine all pages into one list\n",
    "    paper = []\n",
    "    for page in reader.pages:\n",
    "        # extract text from page\n",
    "        page_text = page.extract_text()\n",
    "\n",
    "        # append to paper\n",
    "        paper.append(page_text)\n",
    "\n",
    "    # convert list into string\n",
    "    paper_one_string = ' '.join(paper)\n",
    "    paper_one_string = preprocess(paper_one_string)\n",
    "\n",
    "    return(paper_one_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk and embed function\n",
    "def vectorize(prepared_pdf):\n",
    "    chunked = chunker.chunk(prepared_pdf)\n",
    "    chunk_texts = [chunk.text for chunk in chunked]\n",
    "    chunk_embeddings = model.encode(chunk_texts)\n",
    "    return(chunk_embeddings, chunk_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = prepare_PDF(test_file)\n",
    "test = vectorize(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Last 2 dimensions of the array must be square",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Steven\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:1481\u001b[0m, in \u001b[0;36meig\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m   1479\u001b[0m a, wrap \u001b[38;5;241m=\u001b[39m _makearray(a)\n\u001b[0;32m   1480\u001b[0m _assert_stacked_2d(a)\n\u001b[1;32m-> 1481\u001b[0m \u001b[43m_assert_stacked_square\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1482\u001b[0m _assert_finite(a)\n\u001b[0;32m   1483\u001b[0m t, result_t \u001b[38;5;241m=\u001b[39m _commonType(a)\n",
      "File \u001b[1;32mc:\\Users\\Steven\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:202\u001b[0m, in \u001b[0;36m_assert_stacked_square\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    200\u001b[0m m, n \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;241m!=\u001b[39m n:\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLast 2 dimensions of the array must be square\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Last 2 dimensions of the array must be square"
     ]
    }
   ],
   "source": [
    "linalg.eig(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=test[0]\n",
    "n=m.T@m\n",
    "linalg.eig(n).eigenvectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Vicinity instance\n",
    "vicinity = Vicinity.from_vectors_and_items(vectors=test[0], items=test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<vicinity.vicinity.Vicinity at 0x2585e876540>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vicinity.query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NBER WORKING PAPER SERIES EDUCATION AND ADULT COGNITION IN A LOW-INCOME SETTING: DIFFERENCES AMONG ADULT SIBLINGS Yuan S. Zhang Elizabeth Frankenberg Duncan Thomas Working Paper 32362 http://www.nber.org/papers/w32362 NATIONAL BUREAU OF ECONOMIC RESEARCH 1050 Massachusetts Avenue Cambridge, MA 02138 April 2024 Financial support from the National Institute on Aging (K99/R00 AG070274 [Zhang]) and Eunice Kennedy Shriver National Institute of Child Health and Human Development (P2C HD050924 [Frankenberg] and T32 HD091058 [Zhang]) is gratefully acknowledged The views expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau of Economic Research. NBER working papers are circulated for discussion and comment purposes. They have not been peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies official NBER publications. © 2024 by Yuan S. Zhang, Elizabeth Frankenberg, and Duncan Thomas. All rights reserved. Short sections of text, not to exceed two paragraphs, may be quoted without explicit permission provided that full credit, including © notice, is given to the source.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.tokenize([paper[0]])[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
